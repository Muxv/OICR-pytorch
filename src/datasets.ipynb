{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import copy\n",
    "from utils import VOC_CLASSES\n",
    "from config import cfg\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repetition(boxes):\n",
    "    \"\"\"\n",
    "    remove repetited boxes\n",
    "    :param boxes: [N, 4]\n",
    "    :return: keep:\n",
    "    \"\"\"\n",
    "    _, x1_keep = np.unique(boxes[:, 0], return_index=True)\n",
    "    _, x2_keep = np.unique(boxes[:, 2], return_index=True)\n",
    "    _, y1_keep = np.unique(boxes[:, 1], return_index=True)\n",
    "    _, y2_keep = np.unique(boxes[:, 3], return_index=True)\n",
    " \n",
    "    x_keep = np.union1d(x1_keep, x2_keep)\n",
    "    y_keep = np.union1d(y1_keep, y2_keep)\n",
    "    mask = np.union1d(x_keep, y_keep)\n",
    "    return mask\n",
    "\n",
    "def filter_small_boxes(boxes, min_size):\n",
    "    \"\"\"Filters out small boxes.\"\"\"\n",
    "    w = boxes[:, 2] - boxes[:, 0]\n",
    "    h = boxes[:, 3] - boxes[:, 1]\n",
    "    mask = (w >= min_size) & (h >= min_size)\n",
    "    return mask\n",
    "\n",
    "def totensor(img):\n",
    "    t = transforms.ToTensor()\n",
    "    return t(img)\n",
    "\n",
    "def hflip_box(boxes, w):\n",
    "    for box in boxes:\n",
    "        box[0], box[2] = w - box[2], w - box[0]\n",
    "\n",
    "def hflip_img(img):\n",
    "    \"\"\"\n",
    "    img : PIL Image\n",
    "    \"\"\"\n",
    "#     fliper = transforms.RandomHorizontalFlip(1)\n",
    "#     cflip(src, flipCode[, dst])\n",
    "    return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "\n",
    "def x2ychange_box(boxes):\n",
    "    for box in boxes:\n",
    "        box[0], box[1] = box[1], box[0]\n",
    "        box[2], box[3] = box[3], box[2]\n",
    "\n",
    "def resize_box(boxes, ratio):\n",
    "    for box in boxes:\n",
    "        box[0] = int(ratio * box[0])\n",
    "        box[1] = int(ratio * box[1])\n",
    "        box[2] = int(ratio * box[2])\n",
    "        box[3] = int(ratio * box[3])\n",
    "    \n",
    "def resize_img_smallside(img, smallside):\n",
    "    \"\"\"\n",
    "    img : PIL Image\n",
    "    smallside : change the image small side length to smallside\n",
    "    \"\"\"\n",
    "    w, h = img.size\n",
    "    ratio = 0\n",
    "    resizer = None\n",
    "    if w < h:\n",
    "        if int(smallside*h/w) >= cfg.DATA.MAX_SIDE:\n",
    "            ratio = cfg.DATA.MAX_SIDE/h\n",
    "            resizer = transforms.Resize((int(w*ratio), cfg.DATA.MAX_SIDE))\n",
    "        else:\n",
    "            ratio = smallside/w\n",
    "            resizer = transforms.Resize((smallside, int(h*ratio)))\n",
    "    else: # h <= w\n",
    "        if int(smallside*w/h) >= cfg.DATA.MAX_SIDE:\n",
    "            ratio = cfg.DATA.MAX_SIDE/w\n",
    "            resizer = transforms.Resize((cfg.DATA.MAX_SIDE, (int(h*ratio))))\n",
    "        else:\n",
    "            ratio = smallside/h       \n",
    "            resizer = transforms.Resize((int(w * ratio), smallside))\n",
    "    img = resizer(img)\n",
    "    return img, ratio\n",
    "#     return resizer, ratio\n",
    "\n",
    "class VOCAnnotationAnalyzer():\n",
    "    \"\"\"\n",
    "    deal with annotation data (dict)\n",
    "    \n",
    "    Arguments:\n",
    "        cls_to_idx (dict, optional): dictionary lookup of classnames -> indexes\n",
    "            (default: alphabetic indexing of VOC's 20 classes)\n",
    "        keep_difficult (bool, optional): keep difficult instances or not\n",
    "            (default: False)\n",
    "        height (int): height\n",
    "        width (int): width\n",
    "    \"\"\"\n",
    "    def __init__(self, cls_to_idx=None, keep_difficult=False):\n",
    "        self.cls_to_idx = cls_to_idx or dict(zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n",
    "        self.keep_difficult = keep_difficult\n",
    "        \n",
    "    def __call__(self, annotation: dict):\n",
    "        w = int(annotation['size']['width'])\n",
    "        h = int(annotation['size']['height'])\n",
    "        # if img only contains one gt that annotation['object'] is just a dict, not a list\n",
    "        objects = [annotation['object']] if type(annotation['object']) != list else annotation['object']\n",
    "        res = [] # [xmin, ymin, xmax, ymax, label]\n",
    "        for box in objects:\n",
    "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
    "            difficult = int(box['difficult'])\n",
    "            if not self.keep_difficult and difficult:\n",
    "                continue\n",
    "            name = box['name']\n",
    "            bnd = []\n",
    "            for pt in pts:\n",
    "                bnd.append(int(box['bndbox'][pt]))\n",
    "            bnd.append(self.cls_to_idx[name])\n",
    "            res.append(bnd)\n",
    "            \n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDectectionDataset(data.Dataset):\n",
    "    def __init__(self, root, year, image_set,\n",
    "                 target_transform=VOCAnnotationAnalyzer(),\n",
    "                 dataset_name='VOC07_12',\n",
    "                 region_propose='selective_search',\n",
    "                 debug=False):\n",
    "        super(VOCDectectionDataset, self).__init__()\n",
    "        self.datas = datasets.VOCDetection(root, str(year), image_set, download=False)\n",
    "        self.image_set = image_set\n",
    "        self.name = dataset_name\n",
    "        self.target_transform = target_transform # use for annotation\n",
    "        self.debug = debug\n",
    "        self.region_propose = region_propose\n",
    "        self.box_mat = self.get_mat(year, image_set, region_propose)\n",
    "            \n",
    "            \n",
    "    def get_box_from_mat(self, index):\n",
    "        return self.box_mat['boxes'][0][index].tolist()\n",
    "    \n",
    "    def get_mat(self, year, image_set, region_propose):\n",
    "        \"\"\"\n",
    "        load the box generated\n",
    "        \"\"\"\n",
    "        boxes = None\n",
    "        boxes_score = None\n",
    "        \n",
    "        if str(year) == '2007' and image_set == 'trainval' and region_propose == 'selective_search':\n",
    "            mat = loadmat(\"../region/SelectiveSearchVOC2007trainval.mat\")\n",
    "        elif str(year) == '2007' and image_set == 'test' and region_propose == 'selective_search':\n",
    "            mat = loadmat(\"../region/SelectiveSearchVOC2007test.mat\")\n",
    "        return mat\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        img, gt = self.datas[index]\n",
    "        region = self.get_box_from_mat(index)\n",
    "        if self.target_transform:\n",
    "            gt = self.target_transform(gt[\"annotation\"])\n",
    "        w, h = img.size\n",
    "        \n",
    "        region = np.array(region).astype(np.float32)\n",
    "        region_filter = filter_small_boxes(region, 20)\n",
    "        region = region[region_filter]\n",
    "        \n",
    "        unique_filter = remove_repetition(region)\n",
    "        region = region[unique_filter]\n",
    "        \n",
    "        x2ychange_box(region)        \n",
    "        \n",
    "        gt = np.array(gt).astype(np.float32)\n",
    "\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        if self.debug == True:\n",
    "            return img, gt, region\n",
    "        \n",
    "        elif self.image_set == \"trainval\":\n",
    "            target = [0 for _ in range(len(VOC_CLASSES))]\n",
    "            gt_target = gt[:, -1]\n",
    "            for t in gt_target:\n",
    "                target[int(t)] = 1.0\n",
    "            gt_box = gt[:, :4]\n",
    "            gt_target = np.array(target).astype(np.float32)\n",
    "            \n",
    "                # follow by paper: randomly horiztontal flip and randomly resize\n",
    "            if np.random.random() > 0.5: # then flip\n",
    "                img = hflip_img(img)\n",
    "                hflip_box(region, w)\n",
    "                hflip_box(gt_box, w)\n",
    "            # then resize\n",
    "            max_side = cfg.DATA.SCALES[np.random.randint(5)]\n",
    "            img, ratio = resize_img_smallside(img, max_side)\n",
    "            resize_box(region, ratio)\n",
    "            resize_box(gt_box, ratio)\n",
    "            img = totensor(img)\n",
    "            return img, gt_box, gt_target, region\n",
    "\n",
    "        # ----------------------------------------------------------------------------------\n",
    "\n",
    "        elif self.image_set == \"test\":\n",
    "            n_images = []\n",
    "            n_regions = []\n",
    "            # first change box's cor\n",
    "            # follow by paper: get 10 images for hflip and resize2 5sizes\n",
    "            for scale in cfg.DATA.SCALES:\n",
    "                new_img = img.copy()\n",
    "                new_region = copy.deepcopy(region)\n",
    "                new_img, ratio = resize_img_smallside(new_img, scale)\n",
    "                resize_box(new_region, ratio)\n",
    "                n_images.append(totensor(new_img))\n",
    "                n_regions.append(new_region)\n",
    "\n",
    "            return n_images, gt, n_regions, region          \n",
    "        else:\n",
    "            raise ValueError(f\"image_set can only be 'test' or 'trainval'\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.datas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vd = VOCDectectionDataset(\"~/data/\", 2007, \"trainval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0431, 0.0431, 0.0471,  ..., 0.6392, 0.6353, 0.6353],\n",
       "          [0.0431, 0.0431, 0.0471,  ..., 0.6392, 0.6353, 0.6353],\n",
       "          [0.0392, 0.0392, 0.0431,  ..., 0.6392, 0.6353, 0.6353],\n",
       "          ...,\n",
       "          [0.1882, 0.1961, 0.2118,  ..., 0.2667, 0.2706, 0.2706],\n",
       "          [0.1922, 0.2000, 0.2196,  ..., 0.2667, 0.2706, 0.2706],\n",
       "          [0.1922, 0.2000, 0.2196,  ..., 0.2667, 0.2706, 0.2706]],\n",
       " \n",
       "         [[0.0353, 0.0392, 0.0431,  ..., 0.7294, 0.7255, 0.7255],\n",
       "          [0.0353, 0.0392, 0.0431,  ..., 0.7294, 0.7255, 0.7255],\n",
       "          [0.0353, 0.0392, 0.0431,  ..., 0.7294, 0.7255, 0.7255],\n",
       "          ...,\n",
       "          [0.0627, 0.0667, 0.0706,  ..., 0.3255, 0.3294, 0.3294],\n",
       "          [0.0667, 0.0706, 0.0745,  ..., 0.3255, 0.3294, 0.3294],\n",
       "          [0.0667, 0.0706, 0.0745,  ..., 0.3255, 0.3294, 0.3294]],\n",
       " \n",
       "         [[0.0392, 0.0392, 0.0471,  ..., 0.7529, 0.7490, 0.7490],\n",
       "          [0.0392, 0.0392, 0.0471,  ..., 0.7529, 0.7490, 0.7490],\n",
       "          [0.0392, 0.0392, 0.0471,  ..., 0.7529, 0.7490, 0.7490],\n",
       "          ...,\n",
       "          [0.0157, 0.0196, 0.0314,  ..., 0.4392, 0.4431, 0.4431],\n",
       "          [0.0157, 0.0196, 0.0353,  ..., 0.4392, 0.4431, 0.4431],\n",
       "          [0.0157, 0.0196, 0.0353,  ..., 0.4392, 0.4431, 0.4431]]]),\n",
       " array([[ 841.,  675., 1036., 1084.],\n",
       "        [ 528.,  844.,  809., 1190.],\n",
       "        [ 771.,  620.,  944.,  956.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.], dtype=float32),\n",
       " array([[   3.,    3.,  297.,  774.],\n",
       "        [   3.,    3.,  300.,  780.],\n",
       "        [   3.,    3.,  156.,  784.],\n",
       "        ...,\n",
       "        [ 588., 1129.,  665., 1200.],\n",
       "        [1190., 1132., 1404., 1200.],\n",
       "        [ 892., 1136., 1110., 1200.]], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
